# -*- coding: utf-8 -*-
"""ML_lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XTn27S7roKuQFYcBl-8omCcAnWYyJjbj
"""

def Random_Forest_Model(data):

    # Define features and target
    X=data.iloc[:,:-1].values
    y=data.iloc[:,-1].values

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Create a random forest classifier
    rfc = RandomForestClassifier()

    # Fit the classifier to the training data
    rfc.fit(X_train, y_train)

    # Visualize
    visualize = 5
    estimator = rfc.estimators_[visualize]

    # Plot the tree
    plt.figure(figsize=(100,40))
    plot_tree(estimator, filled=True)
    plt.show()


    # Prediction
    y_pred = rfc.predict(X_test)

    # Accuracy on the train data
    accuracy = rfc.score(X_train, y_train)
    percentage =  accuracy * 100
    print( f"Accuracy on the train data : {percentage} % ")


    # Accuracy on the test data
    accuracy = rfc.score(X_test, y_test)
    percentage =  accuracy * 100
    print( f"Accuracy on the test data : {percentage} % ")

    # Confusion Matrix
    CM = confusion_matrix(y_test,y_pred)
    print( f"Confusion Matrix : \n{CM}")

    # Classification Report
    CR = classification_report(y_test,y_pred)
    print( f"Accuracy on the train data : \n{CR}")

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("drive/My Drive/code_ques.csv")
df.head()

for col in df.columns:
  if df[col].isnull().sum()>0:
    print(col)
    print(df[col].isnull().sum())

df.dropna(inplace=True)

df['Final_score'] = np.where(df['score'] > 5, 1, 0)

X=df.iloc[:,:]
X=X.drop(['score','Final_score'],axis=1)
print(X.shape)
y=df['Final_score']
print(y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

from sklearn import tree
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
def Decision_Tree_Model(X, y):


    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Create a decision tree classifier

    model = DecisionTreeClassifier()
    model.fit(X_train, y_train)
    # Visualize the constructed tree with plot_tree() command
    # Calculate training set accuracy
    train_accuracy = model.score(X_train, y_train)
    print(f"Training Set Accuracy: {train_accuracy}")
    test_accuracy = model.score(X_test, y_test)
    print(f"Test Set Accuracy: {test_accuracy}")

# Get the depth of the tree
    tree_depth = model.get_depth()
    print(f"Tree Depth: {tree_depth}")
    # Assuming 'model' is your trained DecisionTreeClassifier
    plt.figure(figsize=(15, 10))  # Set the figure size (adjust as needed)
    tree.plot_tree(model, filled=True, feature_names=X.columns)  # Adjust feature names if necessary
    plt.show()
    y_train_pred = model.predict(X_train)
    y_train_true = y_train
    y_test_pred = model.predict(X_test)
    y_test_true = y_test
    confusion_matrix_train = confusion_matrix(y_train_true, y_train_pred)
    classification_report_train = classification_report(y_train_true, y_train_pred)

# Confusion Matrix and Classification Report for Test Data
    confusion_matrix_test = confusion_matrix(y_test_true, y_test_pred)
    classification_report_test = classification_report(y_test_true, y_test_pred)

    print("Confusion Matrix for Training Data:")
    print(confusion_matrix_train)
    print("\nClassification Report for Training Data:")
    print(classification_report_train)

    print("\nConfusion Matrix for Test Data:")
    print(confusion_matrix_test)
    print("\nClassification Report for Test Data:")
    print(classification_report_test)

    train_f1_score = f1_score(y_train_true,y_train_pred)
    print("Training F1-Score:", train_f1_score)

    test_f1_score = f1_score(y_test_true,y_test_pred)
    print("Testing F1-Score:", test_f1_score)

    threshold = 0.02

    if abs(train_f1_score - test_f1_score) < threshold:

      outcome = "Regular Fit"

    elif train_f1_score > test_f1_score:

      outcome = "Overfitting"

    else:

      outcome = "Underfitting"



# Print the learning outcome

    print("Model Learning Outcome:", outcome)

Decision_Tree_Model(X, y)

def Decision_Tree_Model_depth(X, y):


    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Create a decision tree classifier

    model = DecisionTreeClassifier(max_depth=5)
    model.fit(X_train, y_train)
    # Visualize the constructed tree with plot_tree() command
    # Calculate training set accuracy
    train_accuracy = model.score(X_train, y_train)
    print(f"Training Set Accuracy: {train_accuracy}")
    test_accuracy = model.score(X_test, y_test)
    print(f"Test Set Accuracy: {test_accuracy}")

# Get the depth of the tree
    tree_depth = model.get_depth()
    print(f"Tree Depth: {tree_depth}")
    # Assuming 'model' is your trained DecisionTreeClassifier
    plt.figure(figsize=(15, 10))  # Set the figure size (adjust as needed)
    tree.plot_tree(model, filled=True, feature_names=X.columns)  # Adjust feature names if necessary
    plt.show()
    y_train_pred = model.predict(X_train)
    y_train_true = y_train
    y_test_pred = model.predict(X_test)
    y_test_true = y_test
    confusion_matrix_train = confusion_matrix(y_train_true, y_train_pred)
    classification_report_train = classification_report(y_train_true, y_train_pred)

# Confusion Matrix and Classification Report for Test Data
    confusion_matrix_test = confusion_matrix(y_test_true, y_test_pred)
    classification_report_test = classification_report(y_test_true, y_test_pred)

    print("Confusion Matrix for Training Data:")
    print(confusion_matrix_train)
    print("\nClassification Report for Training Data:")
    print(classification_report_train)

    print("\nConfusion Matrix for Test Data:")
    print(confusion_matrix_test)
    print("\nClassification Report for Test Data:")
    print(classification_report_test)

    train_f1_score = f1_score(y_train_true,y_train_pred)
    print("Training F1-Score:", train_f1_score)

    test_f1_score = f1_score(y_test_true,y_test_pred)
    print("Testing F1-Score:", test_f1_score)

    threshold = 0.1

    if abs(train_f1_score - test_f1_score) < threshold:

      outcome = "Regular Fit"

    elif train_f1_score > test_f1_score:

      outcome = "Overfitting"

    else:

      outcome = "Underfitting"



# Print the learning outcome

    print("Model Learning Outcome:", outcome)

Decision_Tree_Model_depth(X, y)

def Decision_Tree_Model_entropy(X, y):


    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Create a decision tree classifier

    model = DecisionTreeClassifier(criterion="entropy")
    model.fit(X_train, y_train)
    # Visualize the constructed tree with plot_tree() command
    # Calculate training set accuracy
    train_accuracy = model.score(X_train, y_train)
    print(f"Training Set Accuracy: {train_accuracy}")
    test_accuracy = model.score(X_test, y_test)
    print(f"Test Set Accuracy: {test_accuracy}")

# Get the depth of the tree
    tree_depth = model.get_depth()
    print(f"Tree Depth: {tree_depth}")
    # Assuming 'model' is your trained DecisionTreeClassifier
    plt.figure(figsize=(15, 10))  # Set the figure size (adjust as needed)
    tree.plot_tree(model, filled=True, feature_names=X.columns)  # Adjust feature names if necessary
    plt.show()
    y_train_pred = model.predict(X_train)
    y_train_true = y_train
    y_test_pred = model.predict(X_test)
    y_test_true = y_test
    confusion_matrix_train = confusion_matrix(y_train_true, y_train_pred)
    classification_report_train = classification_report(y_train_true, y_train_pred)

# Confusion Matrix and Classification Report for Test Data
    confusion_matrix_test = confusion_matrix(y_test_true, y_test_pred)
    classification_report_test = classification_report(y_test_true, y_test_pred)

    print("Confusion Matrix for Training Data:")
    print(confusion_matrix_train)
    print("\nClassification Report for Training Data:")
    print(classification_report_train)

    print("\nConfusion Matrix for Test Data:")
    print(confusion_matrix_test)
    print("\nClassification Report for Test Data:")
    print(classification_report_test)

    train_f1_score = f1_score(y_train_true,y_train_pred)
    print("Training F1-Score:", train_f1_score)

    test_f1_score = f1_score(y_test_true,y_test_pred)
    print("Testing F1-Score:", test_f1_score)

    threshold = 0.1

    if abs(train_f1_score - test_f1_score) < threshold:

      outcome = "Regular Fit"

    elif train_f1_score > test_f1_score:

      outcome = "Overfitting"

    else:

      outcome = "Underfitting"



# Print the learning outcome

    print("Model Learning Outcome:", outcome)

Decision_Tree_Model_entropy(X, y)

from sklearn.ensemble import RandomForestClassifier

def Random_forest_model(X, y):


    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Create a decision tree classifier

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    # Visualize the constructed tree with plot_tree() command
    # Calculate training set accuracy
    train_accuracy = model.score(X_train, y_train)
    print(f"Training Set Accuracy: {train_accuracy}")
    test_accuracy = model.score(X_test, y_test)
    print(f"Test Set Accuracy: {test_accuracy}")

# Get the depth of the tree

    y_train_pred = model.predict(X_train)
    y_train_true = y_train
    y_test_pred = model.predict(X_test)
    y_test_true = y_test
    confusion_matrix_train = confusion_matrix(y_train_true, y_train_pred)
    classification_report_train = classification_report(y_train_true, y_train_pred)

# Confusion Matrix and Classification Report for Test Data
    confusion_matrix_test = confusion_matrix(y_test_true, y_test_pred)
    classification_report_test = classification_report(y_test_true, y_test_pred)

    print("Confusion Matrix for Training Data:")
    print(confusion_matrix_train)
    print("\nClassification Report for Training Data:")
    print(classification_report_train)

    print("\nConfusion Matrix for Test Data:")
    print(confusion_matrix_test)
    print("\nClassification Report for Test Data:")
    print(classification_report_test)

    train_f1_score = f1_score(y_train_true,y_train_pred)
    print("Training F1-Score:", train_f1_score)

    test_f1_score = f1_score(y_test_true,y_test_pred)
    print("Testing F1-Score:", test_f1_score)

    threshold = 0.1

    if abs(train_f1_score - test_f1_score) < threshold:

      outcome = "Regular Fit"

    elif train_f1_score > test_f1_score:

      outcome = "Overfitting"

    else:

      outcome = "Underfitting"



# Print the learning outcome

    print("Model Learning Outcome:", outcome)

Random_forest_model(X, y)

# Create a random forest classifier
rfc = RandomForestClassifier(
n_estimators=100,       # Number of decision trees in the forest
max_depth=6,            # Maximum depth of individual trees
min_samples_split=2,    # Minimum number of samples required to split an internal node
min_samples_leaf=1,     # Minimum number of samples required to be at a leaf node
max_features=0.5,       # max_features = 0.5 means it considers a random 50% of the total features at each split.
random_state=42         # Seed for random number generator for reproducibility
)

# Fit the classifier to the training data
rfc.fit(X_train, y_train)


# Prediction
y_pred = rfc.predict(X_test)
y_pred_train = rfc.predict(X_train)

# Visualize
visualize = 5
estimator = rfc.estimators_[visualize]

# Plot the tree
plt.figure(figsize=(50,40))
plot_tree(estimator, filled=True)
plt.show()

# Accuracy on the train data
accuracy = rfc.score(X_train, y_train)
percentage =  accuracy * 100
print( f"Accuracy on the train data : {percentage} % ")


# Accuracy on the test data
accuracy = rfc.score(X_test, y_test)
percentage =  accuracy * 100
print( f"Accuracy on the test data : {percentage} % ")

